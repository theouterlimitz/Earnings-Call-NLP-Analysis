{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7NR5qs0eIRQPER0LW/1hn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theouterlimitz/Earnings-Call-NLP-Analysis/blob/main/01_Data_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZTIsfk8ljl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "AzuJ83txuAjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ** THE FIX IS HERE **\n",
        "# The 'ls -R' command shows the files are in the current directory, so we set the path to '.'\n",
        "path_to_txts = '.'\n",
        "\n",
        "# We will also make the search pattern more specific to only get the Amazon files.\n",
        "all_files = glob.glob(os.path.join(path_to_txts, \"*-AMZN.txt\"))\n",
        "\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"ERROR: Still no Amazon .txt files found in the current directory. Please check that they have been unzipped correctly.\")\n",
        "else:\n",
        "    print(f\"Found {len(all_files)} .txt files to process.\")\n",
        "\n",
        "    # Create an empty list to hold the data for each file\n",
        "    data_records = []\n",
        "\n",
        "    # Loop through the list of filenames\n",
        "    for filename in all_files:\n",
        "        try:\n",
        "            # Extract Info from Filename\n",
        "            base_name = os.path.basename(filename)\n",
        "            parts = base_name.split('.')[0].split('-')\n",
        "\n",
        "            date = \"-\".join(parts[0:3])\n",
        "            ticker = parts[3]\n",
        "\n",
        "            # Read the File Content\n",
        "            with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                transcript_text = f.read()\n",
        "\n",
        "            # Append the structured data to our list\n",
        "            data_records.append({\n",
        "                'ticker': ticker,\n",
        "                'date': date,\n",
        "                'transcript': transcript_text\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process file {filename}. Error: {e}\")\n",
        "\n",
        "    # Concatenate all the records into one master DataFrame\n",
        "    df_master = pd.DataFrame(data_records)\n",
        "\n",
        "    if not df_master.empty:\n",
        "        # Convert the 'date' column to a proper datetime object and sort\n",
        "        df_master['date'] = pd.to_datetime(df_master['date'])\n",
        "        df_master.sort_values(by='date', inplace=True, ignore_index=True)\n",
        "\n",
        "        print(\"\\nSuccessfully combined all .txt files into a single DataFrame.\")\n",
        "\n",
        "        # --- Save the Curated Dataset ---\n",
        "        output_filename = 'amazon_earnings_calls_curated.pkl'\n",
        "        df_master.to_pickle(output_filename)\n",
        "        print(f\"\\nCleaned and consolidated data saved to '{output_filename}'\")\n",
        "\n",
        "        # --- Final Inspection ---\n",
        "        print(\"\\n--- Overview of the Combined Dataset ---\")\n",
        "        print(\"\\n1. Data Info:\")\n",
        "        df_master.info()\n",
        "\n",
        "        print(\"\\n2. First 5 rows of the new DataFrame:\")\n",
        "        print(df_master.drop(columns=['transcript']).head())\n",
        "    else:\n",
        "        print(\"\\nCould not create DataFrame. No files were processed.\")"
      ],
      "metadata": {
        "id": "zYwL28WGvBVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Curation Plus Save**"
      ],
      "metadata": {
        "id": "tS55oDQdj0AY"
      }
    }
  ]
}