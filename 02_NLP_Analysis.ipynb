{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM71PQZUTIgmvgZYt1Pra7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theouterlimitz/Earnings-Call-NLP-Analysis/blob/main/02_NLP_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Your Curated Data**\n"
      ],
      "metadata": {
        "id": "DkysVbi4mh_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckTusNpMmVnC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your curated dataset\n",
        "try:\n",
        "    df_master = pd.read_pickle('amazon_earnings_calls_curated.pkl')\n",
        "    print(\"Curated Amazon earnings call data loaded successfully.\")\n",
        "    print(f\"Loaded {len(df_master)} transcripts.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: 'amazon_earnings_calls_curated.pkl' not found. Please ensure it is uploaded to this session.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Sentiment Analysis**"
      ],
      "metadata": {
        "id": "WsNSi6WemsKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the transformers library from Hugging Face and other dependencies\n",
        "!pip install transformers torch sentencepiece -q\n",
        "\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Load your curated dataset ---\n",
        "try:\n",
        "    df_master = pd.read_pickle('amazon_earnings_calls_curated.pkl')\n",
        "    print(\"Curated Amazon earnings call data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: 'amazon_earnings_calls_curated.pkl' not found. Please ensure it is uploaded.\")\n",
        "    df_master = pd.DataFrame()\n",
        "\n",
        "# ===================================================================\n",
        "# ---  Perform Sentiment Analysis with a Transformer Model ---\n",
        "# ===================================================================\n",
        "\n",
        "if not df_master.empty:\n",
        "    # 1. Load a pre-trained Sentiment Analysis model\n",
        "    print(\"\\nLoading FinBERT sentiment analysis model...\")\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
        "    print(\"Model loaded successfully.\")\n",
        "\n",
        "    # 2. Analyze the sentiment of each transcript\n",
        "    print(\"\\nAnalyzing sentiment for each earnings call transcript...\")\n",
        "\n",
        "    # Convert the transcript column to a list of texts\n",
        "    transcript_list = df_master['transcript'].tolist()\n",
        "\n",
        "    # ** THE FIX IS HERE: **\n",
        "    # Pass the whole list to the pipeline and enable truncation.\n",
        "    # This is faster and correctly handles long texts.\n",
        "    sentiment_results = sentiment_analyzer(transcript_list, truncation=True)\n",
        "\n",
        "    # 3. Add the results back to the DataFrame\n",
        "    print(\"Processing results...\")\n",
        "    df_master['sentiment_label'] = [result['label'] for result in sentiment_results]\n",
        "    df_master['sentiment_score'] = [result['score'] for result in sentiment_results]\n",
        "\n",
        "    # --- Display the Final Results ---\n",
        "    print(\"\\n--- Sentiment Analysis Results ---\")\n",
        "    print(df_master[['date', 'ticker', 'sentiment_label', 'sentiment_score']].head())\n",
        "\n",
        "    # Map labels to numbers for plotting\n",
        "    sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
        "    df_master['sentiment_numeric'] = df_master['sentiment_label'].map(sentiment_mapping)\n",
        "\n",
        "    # Create a weighted score for a more nuanced plot\n",
        "    df_master['sentiment_weighted'] = df_master['sentiment_numeric'] * df_master['sentiment_score']\n",
        "\n",
        "    print(\"\\nPlotting sentiment over time...\")\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    sns.lineplot(data=df_master, x='date', y='sentiment_weighted', marker='o')\n",
        "    plt.axhline(0, color='grey', linestyle='--')\n",
        "    plt.title('Sentiment of Amazon Earnings Calls (2016-2020)', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('Sentiment Score (Positive / Negative)', fontsize=12)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Could not load data to perform analysis.\")"
      ],
      "metadata": {
        "id": "TYFZj23qoWoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarization**"
      ],
      "metadata": {
        "id": "xQ_qC0v5qHuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to import the specific classes from the library\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "\n",
        "# --- Assume df_master is your DataFrame with the transcripts ---\n",
        "# If you are in a new session, you would first load 'amazon_earnings_calls_curated.pkl'\n",
        "# df_master = pd.read_pickle('amazon_earnings_calls_curated.pkl')\n",
        "\n",
        "# ===================================================================\n",
        "# ---  Perform Summarization with Manual Tokenization ---\n",
        "# ===================================================================\n",
        "\n",
        "# 1. Load the pre-trained model and its tokenizer separately\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "print(f\"Loading tokenizer and model for '{model_name}'...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    print(\"Model and tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Stop the script if the model can't be loaded\n",
        "    model = None\n",
        "\n",
        "if model:\n",
        "    # 2. Select a single transcript to summarize\n",
        "    # Let's pick the same transcript from mid-2020\n",
        "    transcript_to_summarize = df_master.loc[18, 'transcript']\n",
        "    transcript_date = df_master.loc[18, 'date'].date()\n",
        "\n",
        "    # 3. Manually tokenize the text with truncation\n",
        "    # This is the key step. We ensure the input is exactly the right size for the model.\n",
        "    # The BART model has a max length of 1024 tokens.\n",
        "    print(f\"\\nTokenizing and truncating the transcript for the earnings call on {transcript_date}...\")\n",
        "    inputs = tokenizer(transcript_to_summarize,\n",
        "                       max_length=1024,\n",
        "                       return_tensors=\"pt\",\n",
        "                       truncation=True)\n",
        "\n",
        "    # 4. Generate the summary from the tokenized input\n",
        "    print(\"Generating summary...\")\n",
        "    summary_ids = model.generate(inputs['input_ids'],\n",
        "                                 num_beams=4,\n",
        "                                 max_length=200,\n",
        "                                 min_length=75,\n",
        "                                 early_stopping=True)\n",
        "\n",
        "    # 5. Decode the summary back into human-readable text\n",
        "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    # --- Display the Results ---\n",
        "    print(\"\\n--- Summarization Results ---\")\n",
        "    print(f\"\\nOriginal Transcript Length (number of words): {len(transcript_to_summarize.split())}\")\n",
        "    print(f\"Generated Summary Length (number of words): {len(generated_summary.split())}\")\n",
        "\n",
        "    print(\"\\n--- GENERATED SUMMARY ---\")\n",
        "    print(generated_summary)"
      ],
      "metadata": {
        "id": "70raitJXqPe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "\n",
        "# --- Assume df_master is your DataFrame with the transcripts ---\n",
        "# If you are in a new session, you would first load 'amazon_earnings_calls_curated.pkl'\n",
        "# df_master = pd.read_pickle('amazon_earnings_calls_curated.pkl')\n",
        "\n",
        "# ===================================================================\n",
        "# --- Perform Advanced Summarization (Robust Method) ---\n",
        "# ===================================================================\n",
        "\n",
        "# 1. Load the pre-trained model and its tokenizer separately\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "print(f\"Loading tokenizer and model for '{model_name}'...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    print(\"Model and tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    model = None\n",
        "\n",
        "if model:\n",
        "    # 2. Select a single transcript to summarize\n",
        "    transcript_to_summarize = df_master.loc[18, 'transcript']\n",
        "    transcript_date = df_master.loc[18, 'date'].date()\n",
        "\n",
        "    # 3. Split the text into manageable chunks\n",
        "    words = transcript_to_summarize.split()\n",
        "    chunk_size = 800\n",
        "    overlap = 100\n",
        "    text_chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size - overlap)]\n",
        "    print(f\"\\nOriginal transcript split into {len(text_chunks)} chunks for processing...\")\n",
        "\n",
        "    # 4. Summarize each chunk individually using the manual method\n",
        "    print(\"Generating intermediate summaries for each chunk...\")\n",
        "    intermediate_summaries = []\n",
        "    for chunk in text_chunks:\n",
        "        # Tokenize each chunk with truncation\n",
        "        inputs = tokenizer(chunk, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
        "        # Generate summary IDs\n",
        "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, min_length=40, early_stopping=True)\n",
        "        # Decode and store the summary\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        intermediate_summaries.append(summary)\n",
        "\n",
        "    # 5. Combine the intermediate summaries into one text block\n",
        "    print(\"Combining intermediate summaries...\")\n",
        "    combined_summary_text = \" \".join(intermediate_summaries)\n",
        "\n",
        "    # 6. Summarize the combined summaries to get the final result\n",
        "    print(\"Generating final executive summary...\")\n",
        "    # Tokenize the combined summary text before final generation\n",
        "    final_inputs = tokenizer(combined_summary_text, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
        "    final_summary_ids = model.generate(final_inputs['input_ids'], max_length=250, min_length=100, do_sample=False)\n",
        "    final_summary = tokenizer.decode(final_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # --- Display the Results ---\n",
        "    print(\"\\n--- Advanced Summarization Results ---\")\n",
        "    print(f\"Date of Earnings Call: {transcript_date}\")\n",
        "    print(f\"Original Transcript Length (words): {len(words)}\")\n",
        "    print(f\"Final Summary Length (words): {len(final_summary.split())}\")\n",
        "\n",
        "    print(\"\\n--- FINAL GENERATED SUMMARY ---\")\n",
        "    print(final_summary)"
      ],
      "metadata": {
        "id": "ywV9Oo8TrvDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# --- Assume df_master is your DataFrame with the transcripts ---\n",
        "# If you are in a new session, you would first load 'amazon_earnings_calls_curated.pkl'\n",
        "# df_master = pd.read_pickle('amazon_earnings_calls_curated.pkl')\n",
        "\n",
        "# ===================================================================\n",
        "# ---  Perform Question Answering with a Transformer Model ---\n",
        "# ===================================================================\n",
        "\n",
        "# 1. Load a pre-trained Question Answering model\n",
        "# This will download the model the first time you run it.\n",
        "print(\"Loading Question-Answering model...\")\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "\n",
        "# 2. Set the context (the full document) and the questions\n",
        "# We'll use the same transcript from July 30, 2020\n",
        "context = df_master.loc[18, 'transcript']\n",
        "transcript_date = df_master.loc[18, 'date'].date()\n",
        "\n",
        "print(f\"\\nAnalyzing transcript from {transcript_date} to answer questions...\")\n",
        "\n",
        "# Define some questions we want to ask\n",
        "question1 = \"How much did AWS grow?\"\n",
        "question2 = \"What was the impact of COVID?\"\n",
        "question3 = \"What is the guidance for Q3?\"\n",
        "\n",
        "# 3. Get the answers from the model\n",
        "print(\"\\n--- Finding Answers ---\")\n",
        "answer1 = qa_pipeline(question=question1, context=context)\n",
        "answer2 = qa_pipeline(question=question2, context=context)\n",
        "answer3 = qa_pipeline(question=question3, context=context)\n",
        "\n",
        "\n",
        "# --- Display the Results ---\n",
        "print(\"\\n--- Question Answering Results ---\")\n",
        "\n",
        "print(f\"\\nQ: {question1}\")\n",
        "print(f\"A: {answer1['answer']} (Confidence: {answer1['score']:.2f})\")\n",
        "\n",
        "print(f\"\\nQ: {question2}\")\n",
        "print(f\"A: {answer2['answer']} (Confidence: {answer2['score']:.2f})\")\n",
        "\n",
        "print(f\"\\nQ: {question3}\")\n",
        "print(f\"A: {answer3['answer']} (Confidence: {answer3['score']:.2f})\")"
      ],
      "metadata": {
        "id": "WKnKFhPnt_hM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}